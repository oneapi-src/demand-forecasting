{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265deea7",
   "metadata": {},
   "source": [
    "## Reference Implementation\n",
    "\n",
    "### E2E Architecture\n",
    "\n",
    "![use_case_flow](assets/e2e-flow-orig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f883764",
   "metadata": {},
   "source": [
    "### Solution Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99933f48",
   "metadata": {},
   "source": [
    "Use the following cell to change to the correct kernel. Then check that you are in the stock kernel. If not, navigate to `Kernel > Change kernel > Python [conda env:stock]`. Note that the cell will remain with * but you can continue running the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05f2da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'conda-env-stock-py'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5176cc",
   "metadata": {},
   "source": [
    "#### Setting up the data\n",
    "\n",
    "Use the `data/generate_data.py` script to generate synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4874a59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd data && python generate_data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0175143",
   "metadata": {},
   "source": [
    "Once we have data, we can view a few samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b9fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect generated data\n",
    "import pandas as pd\n",
    "train_data = pd.read_csv(\"data/demand/train.csv\")\n",
    "train_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74370fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect generated data\n",
    "import pandas as pd\n",
    "test_data = pd.read_csv(\"data/demand/test_full.csv\")\n",
    "test_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dc1fb6",
   "metadata": {},
   "source": [
    "#### Model Building Process\n",
    "\n",
    "We first transform the data to the regression format expected and feed this data into our\n",
    "CNN-LSTM model.  The `run_training.py` script *reads and preprocesses the data*, *trains the model*, and *saves the model* which can be used for future inference.\n",
    "\n",
    "The script takes the following arguments:\n",
    "\n",
    "```shell\n",
    "usage: run_training.py [-h] [-l LOGFILE] [-s SAVE_MODEL_DIR] [-i] [-b BATCH_SIZE]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -l LOGFILE, --logfile LOGFILE\n",
    "                        log file to output benchmarking results to\n",
    "  -s SAVE_MODEL_DIR, --save_model_dir SAVE_MODEL_DIR\n",
    "                        directory to save model to\n",
    "  -i, --intel           use intel configs\n",
    "  -b BATCH_SIZE, --batch_size BATCH_SIZE\n",
    "                        training batch size\n",
    "```\n",
    "As an example of using this to train a model, we can run the following commands from the `/src` directory:\n",
    "\n",
    "```shell\n",
    "conda activate demand_stock\n",
    "python run_training.py --save_model_dir saved_models/stock --batch_size 512\n",
    "```\n",
    "which will produce a saved model in Tensorflow Keras format to the `saved_models/stock` directory which can be used in the next step for running inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e95c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'conda-env-stock-py'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eb7228",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && python run_training.py --save_model_dir saved_models/stock --batch_size 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c778a",
   "metadata": {},
   "source": [
    "### Running Inference\n",
    "\n",
    "The above script will train and save models to the `save_model_dir`.  To use this model to make predictions on new data, a 2-step process is necessary to optimize performance.  \n",
    "\n",
    "1. Convert the saved model from a Keras saved model to a Tensorflow frozen graph.  To do this, we provide a utility script `convert_keras_to_frozen_graph.py` which takes the following arguments:\n",
    "\n",
    "```shell\n",
    "usage: convert_keras_to_frozen_graph.py [-h] -s KERAS_SAVED_MODEL_DIR -o OUTPUT_SAVED_DIR\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -s KERAS_SAVED_MODEL_DIR, --keras_saved_model_dir KERAS_SAVED_MODEL_DIR\n",
    "                        directory with saved keras model.\n",
    "  -o OUTPUT_SAVED_DIR, --output_saved_dir OUTPUT_SAVED_DIR\n",
    "                        directory to save frozen graph to.\n",
    "```\n",
    "\n",
    "For the above saved model, we would run the command\n",
    "\n",
    "```shell\n",
    "python convert_keras_to_frozen_graph.py -s saved_models/stock -o saved_models/stock\n",
    "```\n",
    "which **takes in the saved keras model** and outputs a **frozen graph** in the same directory called `saved_models/stock/saved_frozen_model.pb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08678055",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'conda-env-stock-py'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7d49de",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!cd src && python convert_keras_to_frozen_graph.py -s saved_models/stock -o saved_models/stock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae222fa",
   "metadata": {},
   "source": [
    "2. Once a saved frozen graph is saved, this model can now be used to perform inference using the `run_inference.py` script which has the following arguments:\n",
    "\n",
    "```shell\n",
    "usage: run_inference.py [-h] [-l LOGFILE] [-s SAVED_FROZEN_MODEL] [-b BATCH_SIZE] --input_file INPUT_FILE [--benchmark_mode] [-n NUM_ITERS]\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  -l LOGFILE, --logfile LOGFILE\n",
    "                        log file to output benchmarking results to\n",
    "  -s SAVED_FROZEN_MODEL, --saved_frozen_model SAVED_FROZEN_MODEL\n",
    "                        saved frozen graph.\n",
    "  -b BATCH_SIZE, --batch_size BATCH_SIZE\n",
    "                        batch size to use\n",
    "  --input_file INPUT_FILE\n",
    "  --benchmark_mode      benchmark inference time\n",
    "  -n NUM_ITERS, --num_iters NUM_ITERS\n",
    "                        number of iterations to use when benchmarking\n",
    "```\n",
    "\n",
    "To run inference on a new data file, included for reference as `../data/demand/test_full.csv`:\n",
    "\n",
    "```shell\n",
    "python run_inference.py --input_file ../data/demand/test_full.csv --saved_frozen_model saved_models/stock/saved_frozen_model.pb --batch_size 512\n",
    "```\n",
    "\n",
    "which outputs a json representation of the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ef6f5",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!cd src && python run_inference.py --input_file ../data/demand/test_full.csv --saved_frozen_model saved_models/stock/saved_frozen_model.pb --batch_size 512 --benchmark_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525b3f72",
   "metadata": {},
   "source": [
    "## Optimizing the E2E Reference Solution with Intel® oneAPI\n",
    "\n",
    "#### Model Building Process with Intel® Optimizations\n",
    "\n",
    "The Intel optimizations are enabled by simply using Tensorflow >= v2.9. The `run_training.py` script can be run with no code changes otherwise. The same training process can be run, optimized with Intel® oneAPI as follows:\n",
    "\n",
    "```shell\n",
    "conda activate demand_intel\n",
    "python run_training.py --save_model_dir saved_models/intel --batch_size 512\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178fbc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'conda-env-intel-py'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29562be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && python run_training.py --save_model_dir saved_models/intel --batch_size 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e18db7",
   "metadata": {},
   "source": [
    "#### Model Inference with Intel® Optimizations\n",
    "\n",
    "Similar to model training, the 2 steps of for inference (saving a frozen graph and running inference) is identical with by running the scripts.  Specifically, we can run\n",
    "\n",
    "```shell\n",
    "python convert_keras_to_frozen_graph.py -s saved_models/intel -o saved_models/intel\n",
    "python run_inference.py --input_file ../data/demand/test_full.csv --saved_frozen_model saved_models/intel/saved_frozen_model.pb --batch_size 512\n",
    "```\n",
    "\n",
    "on the saved graph from the above line.  On larger sample data set sizes and more complex models, the gains will become more obvious and apparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96ed57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'conda-env-intel-py'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b69ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && python convert_keras_to_frozen_graph.py -s saved_models/intel -o saved_models/intel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3140df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && python run_inference.py --input_file ../data/demand/test_full.csv --saved_frozen_model saved_models/intel/saved_frozen_model.pb --batch_size 512 --benchmark_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba299af",
   "metadata": {},
   "source": [
    "#### Post Training Optimization with Intel® Neural Compressor\n",
    "\n",
    "In scenarios where the model or data become very large, such as if there are a huge amount of stores and items, and the model is expanded to capture more complex phenomena, it may be desirable to further optimize the latency and throughput of a model.  For these scenarios, one method can utilize *model quantization* techniques via Intel® Neural Compressor.\n",
    "\n",
    "Model quantization is the practice of converting the FP32 weights in Deep Neural Networks to a \n",
    "lower precision, such as INT8 in order **to accelerate computation time and reduce storage\n",
    "space of trained models**.  This may be useful if latency and throughput are critical.  Intel® \n",
    "offers multiple algorithms and packages for quantizing trained models.  In this reference implementation, we \n",
    "include a script, `run_quantize_inc.py` which can be executed *after saving the frozen graph* to attempt to accuracy-aware quantization on the trained model.\n",
    "\n",
    "The `run_quantize_inc.py` script takes the following arguments:\n",
    "\n",
    "```shell\n",
    "usage: run_quantize_inc.py [-h] --saved_frozen_graph SAVED_FROZEN_GRAPH --output_dir OUTPUT_DIR --inc_config_file INC_CONFIG_FILE\n",
    "\n",
    "optional arguments:\n",
    "  -h, --help            show this help message and exit\n",
    "  --saved_frozen_graph SAVED_FROZEN_GRAPH\n",
    "                        saved pretrained frozen graph to quantize\n",
    "  --output_dir OUTPUT_DIR\n",
    "                        directory to save quantized model.\n",
    "  --inc_config_file INC_CONFIG_FILE\n",
    "                        INC conf yaml\n",
    "```\n",
    "\n",
    "which can be used as follows:\n",
    "\n",
    "```shell\n",
    "python run_quantize_inc.py --saved_frozen_graph saved_models/intel/saved_frozen_model.pb --output_dir saved_models/intel --inc_config_file conf.yaml\n",
    "```\n",
    "\n",
    "and outputs a quantized model, if successful, to `saved_models/intel/saved_frozen_int8_model.pb`.  This model is typically smaller at a minor cost to accuracy.  In our case, accuracy falls from a RMSE of 7.59 to an RMSE of 7.60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586365e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'conda-env-intel-py'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3030a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && python run_quantize_inc.py --saved_frozen_graph saved_models/intel/saved_frozen_model.pb --output_dir saved_models/intel --inc_config_file conf.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d4e091",
   "metadata": {},
   "source": [
    "Inference on this newly quantized model can be performed identically as before, pointing the script to the saved quantized graph.\n",
    "\n",
    "```shell\n",
    "python run_inference.py --input_file ../data/test --saved_frozen_model saved_models/intel/saved_frozen_int8_model.pb --batch_size 512\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3616ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd src && python run_inference.py --input_file ../data/demand/test_full.csv --saved_frozen_model saved_models/intel/saved_frozen_int8_model.pb --batch_size 512 --benchmark_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283b125a",
   "metadata": {},
   "source": [
    "## Performance Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583971a9",
   "metadata": {},
   "source": [
    "**Experiment**: Model is trained using `batch_size` 64, 128 and 256, and the model is used for inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6767558c",
   "metadata": {},
   "source": [
    "First, We run training and inference using the stock version.\n",
    "\n",
    "**Change kernel to Python[conda env:stok]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68139731",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'conda-env-stock-py'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0f0bc8",
   "metadata": {},
   "source": [
    "We make sure there are no logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe3b378",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae1f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "if not 'workbookDir' in globals():\n",
    "    import os\n",
    "    workbookDir = os.getcwd()\n",
    "    \n",
    "os.chdir(os.path.join(workbookDir,'src'))\n",
    "from notebooks.utils import benchmarking_utils\n",
    "benchmarking_utils.run_training_benchmark(intel=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f126047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "if not 'workbookDir' in globals():\n",
    "    import os\n",
    "    workbookDir = os.getcwd()\n",
    "    \n",
    "os.chdir(os.path.join(workbookDir,'src'))\n",
    "from notebooks.utils import benchmarking_utils\n",
    "benchmarking_utils.convert_keras_to_frozen_graph_benchmark(intel = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e16106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "if not 'workbookDir' in globals():\n",
    "    import os\n",
    "    workbookDir = os.getcwd()\n",
    "    \n",
    "os.chdir(os.path.join(workbookDir,'src'))\n",
    "from notebooks.utils import benchmarking_utils\n",
    "benchmarking_utils.run_inference_benchmark(intel = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacbd670",
   "metadata": {},
   "source": [
    "Second, we run the raining using Intel® oneAPI Optimizations for Tensorflow to accelerate performance using oneDNN optimizations.\n",
    "\n",
    "**change kernel to Python[conda env:intel]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b1695f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'conda-env-intel-py'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a5e0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "if not 'workbookDir' in globals():\n",
    "    import os\n",
    "    workbookDir = os.getcwd()\n",
    "    \n",
    "os.chdir(os.path.join(workbookDir,'src'))\n",
    "from notebooks.utils import benchmarking_utils\n",
    "benchmarking_utils.run_training_benchmark(intel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcdb0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "if not 'workbookDir' in globals():\n",
    "    import os\n",
    "    workbookDir = os.getcwd()\n",
    "    \n",
    "os.chdir(os.path.join(workbookDir,'src'))\n",
    "from notebooks.utils import benchmarking_utils\n",
    "benchmarking_utils.convert_keras_to_frozen_graph_benchmark(intel=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b64fa27",
   "metadata": {},
   "source": [
    "Finally, We run inference with Intel® oneAPI Optimizations for Tensorflow/Intel® Neural Compressor (inc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b60dc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "if not 'workbookDir' in globals():\n",
    "    import os\n",
    "    workbookDir = os.getcwd()\n",
    "    \n",
    "os.chdir(os.path.join(workbookDir,'src'))\n",
    "from notebooks.utils import benchmarking_utils\n",
    "benchmarking_utils.run_inference_benchmark(intel = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25222ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "if not 'workbookDir' in globals():\n",
    "    import os\n",
    "    workbookDir = os.getcwd()\n",
    "    \n",
    "os.chdir(os.path.join(workbookDir,'src'))\n",
    "from notebooks.utils import benchmarking_utils\n",
    "benchmarking_utils.run_quantize_inc_benchmark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baed9bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "if not 'workbookDir' in globals():\n",
    "    import os\n",
    "    workbookDir = os.getcwd()\n",
    "    \n",
    "os.chdir(os.path.join(workbookDir,'src'))\n",
    "from notebooks.utils import benchmarking_utils\n",
    "benchmarking_utils.run_inference_quantized_model_benchmark(intel=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29f697d",
   "metadata": {},
   "source": [
    "Now, we can create tables and graphs to ilustrate the performance benefits in training and inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c76525",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "Jupyter.notebook.session.restart({kernel_name: 'conda-env-intel-py'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcaf5fd",
   "metadata": {},
   "source": [
    "**Training performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3577056b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "if not 'workbookDir' in globals():\n",
    "    import os\n",
    "    workbookDir = os.getcwd()\n",
    "    \n",
    "os.chdir(os.path.join(workbookDir,'src'))\n",
    "from notebooks.utils import benchmarking_utils\n",
    "benchmarking_utils.print_training_benchmark_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e2a90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "if not 'workbookDir' in globals():\n",
    "    import os\n",
    "    workbookDir = os.getcwd()\n",
    "    \n",
    "os.chdir(os.path.join(workbookDir,'src'))\n",
    "from notebooks.utils import benchmarking_utils\n",
    "benchmarking_utils.print_training_benchmark_bargraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c07995f",
   "metadata": {},
   "source": [
    "**Inference performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03bdd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "if not 'workbookDir' in globals():\n",
    "    import os\n",
    "    workbookDir = os.getcwd()\n",
    "    \n",
    "os.chdir(os.path.join(workbookDir,'src'))\n",
    "from notebooks.utils import benchmarking_utils\n",
    "benchmarking_utils.print_inference_benchmark_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac0409a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "if not 'workbookDir' in globals():\n",
    "    import os\n",
    "    workbookDir = os.getcwd()\n",
    "    \n",
    "os.chdir(os.path.join(workbookDir,'src'))\n",
    "from notebooks.utils import benchmarking_utils\n",
    "benchmarking_utils.print_inference_benchmark_bargraph()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:stock]",
   "language": "python",
   "name": "conda-env-stock-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "069f3196357d110160ed2166d99fab00da978d997cfdeeadaab74238093b8690"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
